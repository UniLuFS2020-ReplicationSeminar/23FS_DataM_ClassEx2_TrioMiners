---
title: "Data Mining in R FS23: Version Control 2"
output: html_document
---
```{r}
library(here)
library(quanteda)
library(quanteda.textplots)
library(tidyverse)
```

# Class Exercise 2

## 1. Introduction
Because of AI recently emerging as a easy accessible tool for everday tasks and thus being a hot topic in society for the last few months, we analyze the press coverage about artificial intelligence from September 2022 to April 2023. We address the questions like "Does the press emphasize more on potential chances or threats of AI/ChatGPT?" or "Does the coverage with time get more negative towards AI?".

Our goal is 
1) To show how sentiment towards AI has shifted in press outlets from September 2022 to April 2023
2) To show how the frequency of press coverage of AI has changed over that same time period

Our hypothesis is that 
a) Overall sentiment of press coverage on AI will be mixed.
b) Frequency of press coverage on AI will increase over time.

## 2. Method
We set out to answer these two questions by first doing a sentiment analysis and secondly by doing a frequency analysis.
But how do we get there?
First, we register for the Guardian API in order to receive an API Key. Next we have to prepare our API Setup, so that we may import the raw data from the API. We set the parameters according to which the extraction should be done. Multiple Loops and extractions take place, resulting in several data and value lists in the R environment.
In step two or rather the second script "data processing", we check for missing values, clean the data, create a corpus and inspect it. The corpus only includes the 200 most relevant articles. After we tokenize the text and remove noise. Once all this preprocessing of data is done, we can export the data and move on to the final step the text analysis.
In our first task, the sentiment analysis, we use the SenitmentAnalysis package. The Sentiment score is defined as the difference between positive and negative word counts divided by the sum of positive and negative words. We group the scores by week and calculate the mean.
Four our second task we simply count the articles over the time period that we are looking at.

## 3. Results
A word clout gives us a first impression of which words appear most in the texts that talk about AI.
```{r}
#load data
load(here("AI_articles_guardian.RData"))
# check for missing values and empty string + cleaning
any(is.na(articles_df))
any_empty_strings <- any(apply(articles_df, 1, function(x) any(nchar(x) == 0)))
any_empty_strings

# cleaning data (removing html tags using regexpr)
articles_df <- articles_df %>% 
  mutate(V3 = gsub("<.*?>", "", V3)) %>% 
  # NAs
  mutate(V1 = if_else(is.na(V1), "MISSING_TEXT", V1)) %>%  # Na handling
  mutate(V2 = if_else(is.na(V2), "MISSING_TEXT", V2)) %>%  # Na handling
  mutate(V3 = if_else(is.na(V3), "MISSING_TEXT", V3)) %>%  # Na handling
  # empty stings
  mutate(across(everything(), ~ if_else(.x == "", "MISSING_TEXT", .x), .names = 'new_{.col}'))

# check for missing values and empty string (again)
any(is.na(articles_df))
any_empty_strings <- any(apply(articles_df, 1, function(x) any(nchar(x) == 0)))
any_empty_strings
# create corpus and inspect it
myCorpus <- corpus(articles_df[1:200,], text_field = "V3") # include only 200 most relevant articles
myCorpus <- corpus(myCorpus, docvars = select(articles_df[1:200,], V1:V3))

myCorpus
tok <- tokens(myCorpus , remove_punct = TRUE, 
              remove_numbers=TRUE, 
              remove_symbols = TRUE, 
              split_hyphens = TRUE, 
              remove_separators = TRUE)
tok <- tokens_remove(tok, stopwords("en"))
tok <- tokens_wordstem(tok)
myDfm <- dfm(tok)
trimdfm <- dfm_trim(myDfm, min_termfreq = 10)
```

```{r}
# word cloud
set.seed(100)
textplot_wordcloud(trimdfm, min_count = 100, 
                   color = c('red', 'pink', 'green', 'purple', 'orange', 'blue'))
```
We can also see this in the form of frequency bars.
```{r}
# frequency bars
library("quanteda.textstats")

features_dfm <- textstat_frequency(trimdfm, n = 100)

# Sort by reverse frequency order
features_dfm$feature <- with(features_dfm, reorder(feature, -frequency))

ggplot(features_dfm, aes(x = feature, y = frequency)) +
  geom_col(fill = "steelblue") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Corpus Top 100 features",
       x = "Feature",
       y = "Frequency")
```
This shows us, that a few relevant key words, such as "ai" and "chatgpt" appear more often than others, but doesn't tell us much else.
For our goals more relevant are the "sentiment over time" and the "number of articles over time" plot.
```{r}
library(here)
library(tidyverse)
library(quanteda)
library(SentimentAnalysis)
library(ggExtra)
### sentiment analysis

#load data
load(here("AI_dfm_guardian.RData"))


# accessing articles and dates in the dfm / convert date variable

articles <- trimdfm@docvars$V3 # note the unusual indexing with @!

dates <- trimdfm@docvars$V1

df_ai <- data.frame(dates, articles)

df_ai <- df_ai %>% 
  mutate(dates = substr(dates, 1, 10)) %>% 
  mutate(dates = as.Date(dates))

# check class of dates
class(df_ai$dates[1])



# load LSD dictionary from quanteda package
LSD_dict <- SentimentDictionaryBinary(data_dictionary_LSD2015$positive, data_dictionary_LSD2015$negative)


# analyze headline sentiment using LSD dictionary (using SentimentAnalysis package)
sentiment <- analyzeSentiment(df_ai$articles, removeStopwords = T, stemming = T,
                              rules=list("Sentiment"=list(ruleSentimentPolarity, # Sentiment score defined as the difference between positive and negative word counts divided by the sum of positive and negative words.
                                                          LSD_dict)))

# add sentinemt to df
df_ai <- data.frame(df_ai, sentiment)


# add column to show week number
df_ai$week_num <- strftime(df_ai$dates, format = "%V")


# group by week and calculate mean of sentiment score
mean_sentiment <- df_ai %>% 
  group_by(week_num) %>% 
  summarise(mean_sent = mean(Sentiment, na.rm = T))


# order weeks correctly
mean_sentiment$week_num <- as.numeric(mean_sentiment$week_num)
class(mean_sentiment$week_num[1])

mean_sentiment$week_num <- factor(mean_sentiment$week_num,levels = c(seq(35,52), seq(1,16)))

```

```{r}


# plot sentiment over time
ggplot(mean_sentiment) +
  geom_col(mapping = aes(x=week_num,y=mean_sent), fill = "steelblue") +
  labs(title = "Sentiment of Coverage about AI from September 2022 to April 2023",
       x = "Week code",
       y = "Sentiment Score")
```

```{r}
### frequency analysis

# count articles over time (order weeks correctly in a first step)
df_ai$week_num <- as.numeric(df_ai$week_num)
df_ai$week_num <- factor(df_ai$week_num,levels = c(seq(35,52), seq(1,16)))

table_count <- table(df_ai$week_num)

# plot number of articles over time
plotCount(table_count, fill = "steelblue") +
  labs(title = "Frequency of Coverage about AI from September 2022 to April 2023",
       x = "Week code",
       y = "Number of Articles")
```
From the first plot we can infer that overall sentiment of press coverage towards AI is positive with a few exceptions. And the second plot shows that over time the AI topic got persistently more and more press coverage.

## 4. Discussion
As we expected the AI topic got more and more press coverage over time, as public interest on the topic increased. The biggest spike in interest and press coverage coincides with the public release of OpenAI ChatGPT 3.0, as large portions of the public began to realize for the first time how much disruptional potential this technology had.
What we didn't expect, was that overall sentiment scores on AI would be largely positive. We believed the topic to be polarizing, exciting some, while frightening others, and therefore resulting in a mixed sentiment score. However, if the graph is to be believed, overall sentiment on the AI topic in the press is slightly positive.
